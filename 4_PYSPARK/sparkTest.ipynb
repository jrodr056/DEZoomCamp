{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4d8c3b-7eae-407a-a59c-91f2c4bdff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d565ef51-ac81-499c-ac56-0d4955eb6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5155705-0975-4ef8-9834-c744a736c567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connecting to d37ci6vzurychx.cloudfront.net (18.164.173.226:443)\n",
      "saving to 'fhvhv_tripdata_2023-01.parquet'\n",
      "fhvhv_tripdata_2023-   0% |                                | 2948k  0:02:36 ETA\n",
      "fhvhv_tripdata_2023-   3% |*                               | 14.1M  0:01:01 ETA\n",
      "fhvhv_tripdata_2023-   5% |*                               | 25.4M  0:00:50 ETA\n",
      "fhvhv_tripdata_2023-   8% |**                              | 36.6M  0:00:45 ETA\n",
      "fhvhv_tripdata_2023-  10% |***                             | 47.9M  0:00:42 ETA\n",
      "fhvhv_tripdata_2023-  13% |****                            | 59.1M  0:00:39 ETA\n",
      "fhvhv_tripdata_2023-  15% |****                            | 70.4M  0:00:37 ETA\n",
      "fhvhv_tripdata_2023-  18% |*****                           | 81.7M  0:00:36 ETA\n",
      "fhvhv_tripdata_2023-  20% |******                          | 92.9M  0:00:34 ETA\n",
      "fhvhv_tripdata_2023-  23% |*******                         |  104M  0:00:33 ETA\n",
      "fhvhv_tripdata_2023-  24% |*******                         |  112M  0:00:33 ETA\n",
      "fhvhv_tripdata_2023-  27% |********                        |  123M  0:00:31 ETA\n",
      "fhvhv_tripdata_2023-  29% |*********                       |  135M  0:00:30 ETA\n",
      "fhvhv_tripdata_2023-  32% |**********                      |  146M  0:00:29 ETA\n",
      "fhvhv_tripdata_2023-  34% |***********                     |  157M  0:00:27 ETA\n",
      "fhvhv_tripdata_2023-  37% |***********                     |  169M  0:00:26 ETA\n",
      "fhvhv_tripdata_2023-  39% |************                    |  180M  0:00:25 ETA\n",
      "fhvhv_tripdata_2023-  42% |*************                   |  191M  0:00:24 ETA\n",
      "fhvhv_tripdata_2023-  44% |**************                  |  202M  0:00:23 ETA\n",
      "fhvhv_tripdata_2023-  47% |***************                 |  214M  0:00:22 ETA\n",
      "fhvhv_tripdata_2023-  49% |***************                 |  225M  0:00:21 ETA\n",
      "fhvhv_tripdata_2023-  51% |****************                |  234M  0:00:20 ETA\n",
      "fhvhv_tripdata_2023-  54% |*****************               |  245M  0:00:19 ETA\n",
      "fhvhv_tripdata_2023-  56% |******************              |  256M  0:00:18 ETA\n",
      "fhvhv_tripdata_2023-  59% |******************              |  268M  0:00:17 ETA\n",
      "fhvhv_tripdata_2023-  61% |*******************             |  279M  0:00:16 ETA\n",
      "fhvhv_tripdata_2023-  64% |********************            |  290M  0:00:14 ETA\n",
      "fhvhv_tripdata_2023-  66% |*********************           |  301M  0:00:13 ETA\n",
      "fhvhv_tripdata_2023-  69% |**********************          |  313M  0:00:12 ETA\n",
      "fhvhv_tripdata_2023-  71% |**********************          |  324M  0:00:11 ETA\n",
      "fhvhv_tripdata_2023-  74% |***********************         |  335M  0:00:10 ETA\n",
      "fhvhv_tripdata_2023-  76% |************************        |  344M  0:00:09 ETA\n",
      "fhvhv_tripdata_2023-  78% |*************************       |  355M  0:00:08 ETA\n",
      "fhvhv_tripdata_2023-  81% |**************************      |  367M  0:00:07 ETA\n",
      "fhvhv_tripdata_2023-  83% |**************************      |  378M  0:00:06 ETA\n",
      "fhvhv_tripdata_2023-  86% |***************************     |  389M  0:00:05 ETA\n",
      "fhvhv_tripdata_2023-  88% |****************************    |  401M  0:00:04 ETA\n",
      "fhvhv_tripdata_2023-  91% |*****************************   |  412M  0:00:03 ETA\n",
      "fhvhv_tripdata_2023-  93% |*****************************   |  423M  0:00:02 ETA\n",
      "fhvhv_tripdata_2023-  96% |******************************  |  434M  0:00:01 ETA\n",
      "fhvhv_tripdata_2023-  98% |******************************* |  446M  0:00:00 ETA\n",
      "fhvhv_tripdata_2023- 100% |********************************|  451M  0:00:00 ETA\n",
      "'fhvhv_tripdata_2023-01.parquet' saved\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2023-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48914e38-33af-4352-a492-1ed704b460e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1530023 fhvhv_tripdata_2023-01.parquet\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2023-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fb36865-fc35-4990-9fc6-e30053ff1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .parquet(\"fhvhv_tripdata_2023-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2da222b8-8d48-46ee-8ecd-57134f0037ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee| tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:18:06|2023-01-01 00:19:24|2023-01-01 00:19:38|2023-01-01 00:48:07|          48|          68|      0.94|     1709|              25.95|  0.0|0.78|      2.3|                2.75|        0.0| 5.22|     27.83|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:48:42|2023-01-01 00:56:20|2023-01-01 00:58:39|2023-01-01 01:33:08|         246|         163|      2.78|     2069|              60.14|  0.0| 1.8|     5.34|                2.75|        0.0|  0.0|     50.15|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:15:35|2023-01-01 00:20:14|2023-01-01 00:20:27|2023-01-01 00:37:54|           9|         129|      8.81|     1047|              24.37|  0.0|0.73|     2.16|                 0.0|        0.0|  0.0|     20.22|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:35:24|2023-01-01 00:39:30|2023-01-01 00:41:05|2023-01-01 00:48:16|         129|         129|      0.67|      431|               13.8|  0.0|0.41|     1.22|                 0.0|        0.0|  0.0|       7.9|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:43:15|2023-01-01 00:51:10|2023-01-01 00:52:47|2023-01-01 01:04:51|         129|          92|      4.38|      724|              20.49|  0.0|0.61|     1.82|                 0.0|        0.0|  0.0|     16.48|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B03406|                NULL|2023-01-01 00:21:34|               NULL|2023-01-01 00:29:05|2023-01-01 00:49:54|         130|          38|     4.921|     1249|              18.29|  0.0|0.43|     1.27|                 0.0|        0.0|  0.0|     16.81|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B03406|                NULL|2023-01-01 00:47:17|               NULL|2023-01-01 00:55:29|2023-01-01 01:16:07|          38|          10|     5.517|     1238|              25.76|  0.0|0.77|     2.29|                 0.0|        0.0|  0.0|     23.65|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:06:54|2023-01-01 00:08:59|2023-01-01 00:10:29|2023-01-01 00:18:22|          90|         231|      1.89|      473|              14.51|  0.0|0.44|     1.29|                2.75|        0.0|  0.0|     13.73|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:15:22|2023-01-01 00:21:39|2023-01-01 00:22:10|2023-01-01 00:33:14|         125|         246|      2.65|      664|               13.0|  0.0|0.39|     1.15|                2.75|        0.0|  0.0|     13.61|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:26:02|2023-01-01 00:39:09|2023-01-01 00:39:09|2023-01-01 01:03:50|          68|         231|      3.26|     1481|              30.38|  0.0|0.91|      2.7|                2.75|        0.0|  0.0|     28.25|                  N|                N|                  |               N|             Y|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:09:35|2023-01-01 00:14:11|2023-01-01 00:14:35|2023-01-01 00:49:13|          79|          50|      3.76|     2078|              19.49|  0.0|0.58|     1.73|                2.75|        0.0|  0.0|     23.13|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:42:57|2023-01-01 00:52:03|2023-01-01 00:52:15|2023-01-01 01:31:11|         143|         223|      7.07|     2336|              58.25|  0.0|1.75|     5.17|                2.75|        0.0|13.58|     43.25|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:17:39|2023-01-01 00:23:21|2023-01-01 00:24:48|2023-01-01 00:37:39|          49|         181|      2.15|      771|              13.04|  0.0|0.39|     1.16|                 0.0|        0.0|  3.0|     10.03|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:41:11|2023-01-01 00:45:57|2023-01-01 00:46:20|2023-01-01 00:52:51|         181|          25|      1.09|      391|              19.59|  0.0|0.59|     1.74|                 0.0|        0.0|  0.0|     21.13|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:44:24|2023-01-01 00:53:17|2023-01-01 00:53:40|2023-01-01 01:31:23|          25|         143|     11.08|     2263|              78.77|  0.0|2.36|     6.99|                2.75|        0.0|  0.0|     50.57|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B03406|                NULL|2023-01-01 00:42:03|               NULL|2023-01-01 00:55:53|2023-01-01 01:26:11|         216|          39|     8.443|     1818|              40.18|  0.0| 1.2|     3.56|                 0.0|        0.0|  0.0|     25.95|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:13:51|2023-01-01 00:26:55|2023-01-01 00:28:05|2023-01-01 00:37:45|         223|           7|      1.65|      580|               9.47|  0.0|0.28|     0.84|                 0.0|        0.0|  0.0|      7.88|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:30:12|2023-01-01 00:38:51|2023-01-01 00:40:51|2023-01-01 00:54:09|           7|         223|      2.02|      798|              21.48|  0.0|0.64|     1.91|                 0.0|        0.0|  0.0|     16.86|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B03404|              B03404|2023-01-01 00:50:41|2023-01-01 00:59:03|2023-01-01 00:59:56|2023-01-01 01:18:47|         223|         145|      3.83|     1131|              18.74|  0.0|0.56|     1.66|                 0.0|        0.0|  0.0|     17.55|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B03406|                NULL|2023-01-01 00:16:47|               NULL|2023-01-01 00:18:44|2023-01-01 00:48:36|          79|         188|    10.141|     1792|              30.49| 0.98|0.95|     2.82|                2.75|        0.0|  0.0|      27.7|                  N|                N|                 N|               N|             N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+-----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261bc15b-f505-40c5-b645-6f940c17d043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B03404', originating_base_num='B03404', request_datetime=datetime.datetime(2023, 1, 1, 0, 18, 6), on_scene_datetime=datetime.datetime(2023, 1, 1, 0, 19, 24), pickup_datetime=datetime.datetime(2023, 1, 1, 0, 19, 38), dropoff_datetime=datetime.datetime(2023, 1, 1, 0, 48, 7), PULocationID=48, DOLocationID=68, trip_miles=0.94, trip_time=1709, base_passenger_fare=25.95, tolls=0.0, bcf=0.78, sales_tax=2.3, congestion_surcharge=2.75, airport_fee=0.0, tips=5.22, driver_pay=27.83, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f362ee1c-e5e3-4ea7-aae3-520405bf98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1001 fhvhv_tripdata_2023-01.parquet > head.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2021a2-dd5a-4347-9c7f-f19bd2c509da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAR1\u0015\u0004\u0015(\u0015BL\u0015\u0004\u0015\u0004\u0012\u0000\u0000\u001f\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "cc``3000fƒ±L\u0001cÀ\u0005w\u0014\u0000\u0000\u0000\u0015\u0000\u0015üöK\u0015Ð¶=,\u0015€€€\u0004\u0015\u0004\u0015\u0006\u0015\u0006\u001c",
      "6\u0000(\u0006HV0005\u0018\u0006HV0003\u0000\u0000\u0000\u001f\u0000\u0000\u0000\u0000\u0000\u0000\n",
      "<ý\n",
      "t\u0014×™'Œ×­[Ý·ÚýQÕj\t•P©«$aD\u001c",
      "\u0002%D e5ê\u0012\"\u0016I\u001c",
      "ãlöìzf=¦e<Æ³ã5­à\n",
      "M�éÛjLËc‚äñ¾k²!\u0016™ìlòžwß\u0019¼Ùÿ†ì8¦šÆˆ\"›=;dÃ˜jäXž�×”¬Lh\u0005¡óžjæ=9\u0001,uWÝ�çã÷|\u0007\u0004AàœK„„³¼Þ»¥I`4KT!Tsh¤I Ãª 0[Õ³[Õ‡…\u0010\u0015YÏ\u001a!Ð\u0019´\u00046›•U¡»*[ª[«\u001bvÄá‡¹¢[Í\u0002e[\u0004–ÉöŒ\u000b",
      "ÔÐ„°Q�8š&ÐÌ?\u0015\u0002”[ðG2)°’¢?$(ýYÍÝÌ\t¼§Ø$\u0004Þ`k…0\u0013'÷µéBü\u0015%Ë�\u0011y%)Ð’.\u0004Øvxpk—ÐL\u0002\u000eyF`ÌÉ¬\u0011Xo¡­IH0iÞUN:ÁìZxw“Ð6K¥fÎ\u0007x~üŸXðØ\u0005²^`Š(i„é9s�@\u0017U�Í‘|³À–ØÇº@©*\u0004*Å5$œ3\u001d",
      "rüa8\u0001)\u0007?\u001aÊÛB¨|šQx\u0005�]wHnå \tK…ÿ\u000e\u0013K36®\u0017\u0002†\u0001ç£P5¦’ÙuB¸o5”TTáAÊ\n",
      "zå†™z<›wŠŠkrE\u0013Bld<\u000b",
      "ï\u001c",
      "»–…‡Ìw\t‰W>sÖ6œ2�5\u0005âã¦\u0010ÈŒZ°\u0003Ø(�Ïe$\u0015öSÓ\u0004%^±Ý1Wn\u0016Z¦r\u001a±¸â~‹¬\u0011Öà±±.¡�ÙÄ’bê³â,á\u0015•\u0004$k£@Ûà\t•\u0012]\u000b",
      "'W€\u001aná!8\u0004×x\n",
      "~ð¼.°BLzDˆÓS†â’Ixe3‰ªu\u0016Ä\u000b",
      "b¥\u0010_#„©§8¸\u000eòžÝ$Ä\u0015'79\u001f©ï…‹4\u001e",
      "‚ËÜ \u0004z¶ÁY\u0012¸Ë!Gi\u0012BÄ-’µB¨Ç3\n",
      "]@3Îl \"«°Ÿ¥f!dLWŒµ°u^YßØ }ì=\t÷\u0010Ž08ê2\\[€Ý�ÿ^àpãJ›yœèY<l\u0002wO‚küS§Ä„ÇY‚R¢­\u0011+Ÿß&\u0004öÇL …)8\u000e\n",
      "ÏAz\u000b",
      "m—ôà\u0006!üíÞ\u0015Ë…GÏý\f",
      "~®à\n",
      "\u0003©°8\u0013×ÀR\u001d",
      "ó£&!Ê*SÔŠ¨d#¼b;|j^\u0013T²Þ§!¤‰è>—ž\"øÞ;¦\u0010/�êd9Ñ5�^\u000e©þr\u0018\u001d",
      "4U!¾8ìˆª¥Ù°?ÂáÀ©±I ³\u001báQ4—\u0014Bwö©I¸ÚVX\u001acåQ\n",
      "öiÀ3J[\u0005Ú¹VPFnÌ:¬=›\u0014¢‡T9&ÚðèÍp\u0000”GOÌJŽ†\u000b",
      "!a³/\u0016\u001c",
      "o\u0016Âå\u0012ýx?œÁ\u001e",
      "xVy\u0004–ÉîÉp&áL.èÀ\u0011°Œe\u0003©\u001bg5Ø.“t!± ÆÞ>c*ÙÎ.!:¤.”X\u001a\u0018²“8Wz“Bë7‰LÇu9Ÿ¤n\u0013Ð’ÒItžZdM$ð\\]‡åÃ\tÜëµ²º\u0004”³ ‰°«\n",
      "¬~$Ò%„•�#¹«ƒB@q€(É)\u0002Œïv¾ùÏá\u0012\u0017¯$…¸Á¥ô%æDàHŠD\\'Äg\u000e«IÝŒ=Ð,´:ëÌOÄs¢|Š½\n",
      "ÜÉŠÝA¸öb±m\n",
      "¼’‰—ƒI¡…Ù\\áµwèjPCq±Eˆ“A¢:míµ�Bxá€rZ\u0007ª\u0018Óa—Ôî\u0012h\u0015Ž }Ž¶ÈMþM'^)8sd­|Ñ�\u000fÜ\tþ3¸ÝQ¸ôÏYŸ˜ð¸í´YˆÖÜb†À\u0006\u0019KM®ƒ§\u0000�‡ÊÄ\u00032\u000eÐÓp…\u00046«à\n",
      "ó\n",
      "r\u0010Cú\u0003Â (`\u001e",
      "\u0006zhòÿ\u001d",
      " \u0016Ü¤ÒåÿGb(æ…x^N\u0006Aˆ•^¯)§t¡ã^^£õ\u0012óh2ëd<ÕgìÀ¿†Kfóã ]™²d®\u0015¢¥™¤K\u00065¡…®ª_·�J¨â¡D\\\u0007\u0004õ±\u0001\u00175KÉ\u0007±—é>B´&¤t¸,\u0006\u0007\u000b",
      "TE)r.\u0005ò\t°£ÀVÃë…è\u001e",
      "·>GbpŸ•Í’ºB\\`e\u0012ç¶¾Vh�k^vµ|ðô\u001f¤‚œ\u0001\u0017+@iÄ!Î\u001c",
      "gšÐ\u001a\u001a¢ÎµùÜ\u0019³>\f",
      "çP¤ ¡Â—§Y^Í¥% >\u00129Õ$Â—~\u0002Òª´‚\u0014&‚ ,Îå,\u0015x‰¸ÿ¿ãÊ5ØXû|\u0018ˆy!§‹Ü€“žê+ÒIàý\u000e2[¦¼\u0014He�ÊÌãú\u0006�–m¸V\u001d",
      "\u0004Î-ç(l…¡$\u0019•»á\u0011}Î)UhýUQtF&Úx©Ê¡/§‡;š€ÖbéßE‚ãY÷(J\u001c",
      "U#ð (\\.ËŠ cB;êT�‘›…Ï$$Ñ±V‰£ÿ;ZˆYª¨ÏÛJb\u0018È\u001fe3�\\+!Ùú\u0014#î‰”Ž²à½{äCª\t\u001d",
      "l èÖùül½\u001a\u000b",
      "¾\u000f«¡ÃÍ@(ôû‘ë@vŠ\n",
      "ŒFU¸·„\"ƒÌë·Ò$\n",
      "k‘Ìu@DO®\u0016L!z‡œ)»%äuW\u0017¢?TÓ'ËS ™?d¦\u0010š�%EøäØq«°\u0015”\f",
      "l; Ø@\u0016‰:\u0010Î€´[\u001d",
      "lÂ\u0017.n‚Ÿ'�à‘Ò�Jq­\u0010ÿÕÏÊ�Zä(H\u001d",
      "¤yúå5B\u0013|\u001d",
      "é!P\f",
      "¢\"\u0003þZ¾x[ÔuÏž¤v…l\u0015Zf˜mªò.c\u0005‰»ju\t­¯�y½�ñJ\"ã&�˜ª(—\u001c",
      "Ë—\u0018@*ýýGe¸Wvð\u0003�¾¡YE¹\u0001Ÿ\u0019°àúW½èõ5B»â2­>|‘Fþ¬Øm‚äœCí‰L\u001e",
      "ª²tp£ÐÖ.Åb'\u0016\tOçx?(Ž¸Ê'é£Ï8(…ÊX¶®\u0006‡V@6\u0019z:¸ïñ!\u001d",
      "Ù�>¶Ò,(½iÙ³V�j\u0002TRIKW¹›\u0013Ñ:\u0012Y\u0001nØ\f",
      "‡i\u0000[Ä·[+J†³8üs¤>QÕ½›\u001c",
      "\u001e",
      "=\u0007r¯FSðU6\n",
      "ûì‰¬\u0011âmë²ƒé1)\u0003rl¹²›Û�mh°Î\u0014\u001b…uïù\u0004õ¿Ñ\n",
      "²ƒ-”_FÈóŠ²\u0014CNGe‚l\u0006\u001b\u001bƒ‡‡¶; W…ÀÍCðÎ}NÍÐÇNÂC\u0003·Q\u0001�l�˜\u0012HƒQkƒ\u0010�r¾?ÂK�Â©ÒÕ'7\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 head.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d7f273-b26a-4f96-a13c-f6d2ee59aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff0bd64-b066-4d95-ad3d-ae53abfac56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_parquet(\"fhvhv_tripdata_2023-01.parquet\",engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "037616ff-80e0-44de-8fe0-aaaa2f09ca94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "originating_base_num            object\n",
       "request_datetime        datetime64[ns]\n",
       "on_scene_datetime       datetime64[ns]\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "trip_miles                     float64\n",
       "trip_time                        int64\n",
       "base_passenger_fare            float64\n",
       "tolls                          float64\n",
       "bcf                            float64\n",
       "sales_tax                      float64\n",
       "congestion_surcharge           float64\n",
       "airport_fee                    float64\n",
       "tips                           float64\n",
       "driver_pay                     float64\n",
       "shared_request_flag             object\n",
       "shared_match_flag               object\n",
       "access_a_ride_flag              object\n",
       "wav_request_flag                object\n",
       "wav_match_flag                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cacdf9-e8cf-4109-bfb4-7ada41da9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "326f16fc-c8bd-4e35-a4f8-d9f1a9da1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4107572b-cb7e-4b4c-a0a0-ea6317b123d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    types.StructField(\"hvfhs_license_num\",types.StringType(),True),\n",
    "    types.StructField(\"dispatching_base_num\",types.StringType(),True),\n",
    "    types.StructField(\"originating_base_num\",types.StringType(),True),\n",
    "    types.StructField(\"request_datetime\",types.StringType(),True),\n",
    "    types.StructField(\"on_scene_datetime\",types.StringType(),True),\n",
    "    types.StructField(\"pickup_datetime\",types.TimestampType(),True),\n",
    "    types.StructField(\"dropoff_datetime\",types.TimestampType(),True),\n",
    "    types.StructField(\"PULocationID\",types.IntegerType(),True),\n",
    "    types.StructField(\"DOLocationID\",types.IntegerType(),True),\n",
    "    types.StructField(\"trip_miles\",types.StringType(),True),\n",
    "    types.StructField(\"trip_time\",types.StringType(),True),\n",
    "    types.StructField(\"base_passenger_fare\",types.StringType(),True),\n",
    "    types.StructField(\"tolls\",types.StringType(),True),\n",
    "    types.StructField(\"sales_tax\",types.StringType(),True),\n",
    "    types.StructField(\"congestion_surcharge\",types.StringType(),True),\n",
    "    types.StructField(\"airport_fee\",types.StringType(),True),\n",
    "    types.StructField(\"tips\",types.StringType(),True),\n",
    "    types.StructField(\"driver_pay\",types.StringType(),True),\n",
    "    types.StructField(\"shared_request_flag\",types.StringType(),True),\n",
    "    types.StructField(\"shared_match_flag\",types.StringType(),True),\n",
    "    types.StructField(\"access_a_ride_flag\",types.StringType(),True),\n",
    "    types.StructField(\"wav_request_flag\",types.StringType(),True),\n",
    "    types.StructField(\"wav_match_flag\",types.StringType(),True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b45e75d1-e960-45e6-a4a3-bcfa16edb4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .parquet(\"fhvhv_tripdata_2023-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4410e1ca-b238-4ffb-b61e-99829a3ff9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B03404', originating_base_num='B03404', request_datetime=datetime.datetime(2023, 1, 1, 0, 18, 6), on_scene_datetime=datetime.datetime(2023, 1, 1, 0, 19, 24), pickup_datetime=datetime.datetime(2023, 1, 1, 0, 19, 38), dropoff_datetime=datetime.datetime(2023, 1, 1, 0, 48, 7), PULocationID=48, DOLocationID=68, trip_miles=0.94, trip_time=1709, base_passenger_fare=25.95, tolls=0.0, bcf=0.78, sales_tax=2.3, congestion_surcharge=2.75, airport_fee=0.0, tips=5.22, driver_pay=27.83, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74f8d9a7-b9a6-4d5b-aef2-6c36d07f1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0215f334-01f5-424a-9795-7affcec64e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('fhvhv/2023/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68d47aaa-7a64-4565-b367-a5add080290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2023/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a09240b-1ad6-4751-87c7-62dfcd2d732b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp_ntz (nullable = true)\n",
      " |-- on_scene_datetime: timestamp_ntz (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27d6e2ac-dcb8-4f4b-9243-d0efb27ff638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "777c9af7-f070-4ef0-a243-a5c37baea566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f'/s{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28a9f86e-5e0b-4002-9f09-8de98498c29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sb44'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_stuff('B02884')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5699b945-9f8f-4d27-919c-8e13a0f9ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazyStuffUDF = F.udf(crazy_stuff, returnType=types.StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb9695-9a05-41c5-bfa0-8af31ed55ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c6fe287-0a20-4c62-845e-9760342df348",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o378.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 255) (DESKTOP-2C1O2P8 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:492)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:153)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:492)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:153)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpickup_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickup_datetime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropoff_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropoff_datetime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcrazyStuffUDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatching_base_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpickup_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropoff_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPULocationID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDOLocationID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\CODING\\packages\\spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\CODING\\packages\\spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\CODING\\packages\\spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mD:\\CODING\\packages\\spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o378.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 255) (DESKTOP-2C1O2P8 executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:492)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:153)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:181)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:492)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:153)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date',F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date',F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id',crazyStuffUDF(df.dispatching_base_num)) \\\n",
    "    .select('base_id','pickup_date','dropoff_date','PULocationID','DOLocationID') \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17b42aba-b2b6-4222-a540-466bfe080f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2023-01-15 17:03:04|2023-01-15 17:35:48|         244|         211|\n",
      "|2023-01-18 06:47:42|2023-01-18 06:54:09|          68|         234|\n",
      "|2023-01-09 18:26:03|2023-01-09 18:48:06|         122|          82|\n",
      "|2023-01-26 09:42:16|2023-01-26 09:53:05|         196|          95|\n",
      "|2023-01-27 12:03:06|2023-01-27 12:07:36|         223|         223|\n",
      "|2023-01-24 13:54:14|2023-01-24 14:36:07|         241|         215|\n",
      "|2023-01-02 11:07:23|2023-01-02 11:20:21|         161|         246|\n",
      "|2023-01-20 11:21:32|2023-01-20 11:37:28|         159|          74|\n",
      "|2023-01-11 01:20:06|2023-01-11 01:24:42|          37|          37|\n",
      "|2023-01-02 17:11:12|2023-01-02 17:33:08|          45|          49|\n",
      "|2023-01-01 05:33:14|2023-01-01 06:02:12|          80|         132|\n",
      "|2023-01-25 20:10:58|2023-01-25 20:19:12|         234|         164|\n",
      "|2023-01-18 12:40:19|2023-01-18 12:52:00|         169|         248|\n",
      "|2023-01-29 00:35:28|2023-01-29 00:56:38|         249|          97|\n",
      "|2023-01-22 09:36:58|2023-01-22 09:53:48|         255|         164|\n",
      "|2023-01-02 17:19:58|2023-01-02 17:26:46|          72|          72|\n",
      "|2023-01-07 17:57:24|2023-01-07 18:03:16|          36|          36|\n",
      "|2023-01-23 16:23:11|2023-01-23 16:37:14|         144|         256|\n",
      "|2023-01-27 06:50:27|2023-01-27 07:24:08|          80|          61|\n",
      "|2023-01-19 23:15:30|2023-01-19 23:25:23|          76|          76|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime','dropoff_datetime','PULocationID','DOLocationID').filter(df.hvfhs_license_num=='HV0003').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a459040d-53e3-43e2-9c84-7e08d2728019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "head: head.csv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9171478-4bf6-425d-8060-e3c2600b9a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
